<!DOCTYPE html>
<html lang="pl">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Instrukcje</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Nunito:300,400,700" rel="stylesheet">
    <!-- Link to your stylesheet here -->
    <link rel="stylesheet" href="lit.css">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <style>
    .example-grid .row {
        text-align: center;
        font-size: .8em;
        letter-spacing: .1rem;
        margin: .8em 0;
    }

    .example-grid .col {
        background: #EEE;
        padding: .3em;
    }

    DIV.cytat {text-align: right}
    </style>
</head>

<body class="c">

    <div class="row">
        <div class="9 col">
            <h1>Instrukcje</h1>
            <h5>Instrukcje do aplikacji działających na serwerze ai.ihpan.edu.pl</h5>
        </div>
        <div class="3 col">
            <div class="cytat"><img src="logo-ihpan.png" width="50"></div>
        </div>
    </div>

    <hr>

    <div class="row">
        <div class="9 col">

        <h2>OpenWebUI</h2>

        <h4>1. Opis</h4>

        <p>OpenWebUI jest oprogramowaniem typu <a href="https://github.com/open-webui/open-webui">open source</a> będącym webowym interfejsem typu CHAT do modeli LLM. Aplikacja pozwala nie tylko na rozmowę z modelami, ale także dzięki wbudowanym mechanizmom RAG (Retrieval-Augmented Generation) użytkownicy mogą wprowadzać treść dokumentów i całych kolekcji dokumentów lub wyniki wyszukiwania web bezpośrednio do rozmowy, co wzbogaca kontekst i odpowiedzi modeli. Interfejs OpenWebUI pozwala na modyfikacje parametrów pracy modelu, zapisywanie treści rozmów w plikach pdf, json, txt. Dane używane podczas rozmowy nie są wysyłane na zewnątrz, chyba że wykorzystywany jest model z zewnętrznego a nie lokalnego serwera. Aplikacja działa na komputerach stacjonarnych, laptopach i urządzeniach mobilnych.</p>


        <h4>2. Modele LLM dostępne w aplikacji OpenWebUI pochodzą z 3 źródeł:</h4>

        <ul>
            <li> <strong>lokalne</strong>, uruchamiane na serwerze <strong>ai.ihpan.edu.pl</strong> (te działają dość wolno, nie mamy jeszcze sprzętu odpowiedniego do takich zadań, dodatkowo modele lokalne to wersje skwantyzowane do 4 bitów - nieco uproszczone - by mogły działać na samych CPU)</li>

            <li> <strong>serwer API GroqCloud</strong> - serwer udostępniający modele poprzez api, ta firma nie korzysta kart GPU Nvidii ale z własnych układów LPU, używanych tylko do inferencji (odpytywania modeli), nie nadają się do trenowania czy fine-tuningu. API jest bezpłatne, ale ma dzienne ograniczenia, jest możliwe że w pewnym momencie model  z tego źródła przestanie odpowiadać i trzeba będzie poczekać do następnego dnia.</li>

            <li> <strong>serwer API OpenAI</strong> - komercyjny dostęp przez API do modeli GPT firmy OpenAI, modele z tgo źródła będą dostępne aż do wyczerpania (niewielkiego) kredytu.</li>
        </ul>

        <h4>3. Lista dostępnych modeli:</h4>

        <ul>
            <li><strong>llama-3.2-90b-text-preview</strong> (Groq Cloud), model open-source od formy Meta, domyślnym modelem używany w czacie</li>

            <li><strong>gemma2-9b-it</strong> (Groq Cloud) lekki, otwarty model od Google, stworzony na podstawie tych samych badań i technologii, które posłużyły do ​​stworzenia modeli Gemini (ale sporo mniejszy)</li>

            <li><strong>gpt-4o-mini</strong> (OpenAI), mały model GPT firmy OpenAI (ale i tak lepszy od większości modeli open-source)</li>

            <li><strong>mixtral-8x7b-32768</strong> (Groq Cloud), model open-source francuskiej firmy Mistral aI</li>

            <li><strong>bielik-11b-v2.3-instruct</strong> (lokalny) - model open-source wytrenowany na korpusie polskich tekstów przez społeczność zorganizowaną wokół stowarzyszenia SpeakLeash oraz specjalistów z Cyfronet AGH. Model oparty na innym modelu open-source: Mistral-7B.</li>

            <li><strong>Regulamin AI</strong> - model 'dostosowany' - to definicja modelu opartego na llama-3.2-90b-text-preview z przypisanym odpowiednim promptem systemowym i bazą wiedzy (kolekcją dokumentów - w tym przypadku to regulaminy i zarządzenia obowiązujące w IH PAN. Uwaga: na razie w kolekcji są tylko wybrane regulaminy). Model odpowiada na tematy związane z regulaminami Instytutu.</li>

            <li><strong>PSB AI</strong> - model 'dostosowany' - odpowiada na pytania na podstawie kolekcji biogramów PSB (obecnie z tomu 1).</li>

            <li><strong>Bałtyk AI</strong> - model 'dostosowany' - odpowiada na pytania dotyczące polityki morskiej Jagiellonów w XVI wieku, na podstawie kolekcji publikacji będących już w domenie publicznej (głównie autorstwa Stanisława Bodniaka)</li>
        </ul>

        <h4>4. Praca z OpenWebUI</h4>

        <p>Po zalogowaniu widoczny jest interfejs czatu. W polu na treść promptu (pytania) widoczna jest podpowiedź: "Jak mogę Ci dzisiaj pomóc?", po lewej stronie widoczny jest znak + który wywoluje menu, dzięki któremu można załączyć plik (txt, pdf) lub włączyć wyszukiwanie w internecie. Poniżej widoczna jest lista typowych pytań które można zadać modelowi jednym kliknięciem.</p>

        <p>Po lewej stronie ekranu widoczna jest lista poprzednich czatów (początkowo oczywiście pusta), u góry zaś przycisk do tworzenia nowego czatu, pole opcji z możliwością ustawienia aktywnego modelu. Można też dodać kolejny aktywny model. Aplikacja zapyta wówczas każdy z nich i wyświetli wiele odpowiedzi.</p>
        <p>U góry po prawej stronie ekranu widoczne są trzy ikony. Pierwsza z trzema kropkami dotyczy opcji czatu, można:

        <ul>
            <li>udostępnić czat w sieci - aplikacja utworzy link, ale by go użyć należy mieć konto w danej instancji OpenWebUI</li>
            <li>skopiować treść czatu do schowka (w formacie markdown)</li>
            <li>wyświetlić czat w formie diagramu (Chat Overview)</li>
            <li>pobrać czat w formacie: txt, json, pdf.</li>
        </ul>
        </p>

        <p>Druga z ikon (ikona z poziomymi suwakami) pozwala na modyfikacje parametrów czatu, np. ustawienie promptu systemowego (można w nim np. wskazać rolę którą ma odgrywać model, choćby taką: "Jesteś asystentem osób badających historię Polski, specjalistą w dziedzinie biografii postaci historycznych."), ale także zaawansowanych technicznych parametrów modelu np. temperatura, domyślnie posiadająca wartość 0.8 (czyli model jest dość 'kreatywny'), po zmianie na wartość 0.0 model powinien być mniej twórczy, ale mieć mniejsze skłonności do halucynacji.</p>

        <p>Ostatnia z górnych ikon związana jest z bieżącym użytkownikiem i pozwala na wyświetlenie okna z ustawieniami użytkownika oraz na wylogowanie się z aplikacji (identyczną funkcjonalność ma przycisk z nazwą użytkownika w dolnym lewym rogu ekranu).</p>

        <p>Ustawienia użytkownika pozwalają na wybór domyślnego modelu dla bieżącego użytkownika, język interfejsu czy też zmianę hasła dla konta.</p>

        <p>Sesję czatu z modelem można rozpocząć od wyboru modelu z menu u góry ekranu, następnie do rozmowy można (opcjonalnie) załączyć plik np. z lokalnego dysku (znak + i opcja Prześlij pliki), lub załączyć kolekcję dokumentów (bazę wiedzy) wpisując w polu na prompt znak #, który wywołuje listę dostępnych kolekcji dokumentów. Po ustawieniu pliku lub kolekcji będą one dostępne podczas całego czatu z modelem. Następnie można zadać pytanie, lub zdefiniować zadanie dla modelu. Działa też nagrywanie pytania głosem - aplikacja wykonuje transkrypcję (ale raczej w języku angielskim). Po wpisaniu pytania przycisk ze strzałką wysyła wiadomość do modelu. Czas oczekiwania na odpowiedź może być różny, zależnie od tego czy przeszukiwana jest kolekcja dokumentów, czy sieć internet, czy wykorzystywany jest model z szybkiego serwera zewnętrznego, czy też z serwera lokalnego. Zwykle oczekiwanie trwa od kilku sekund do paru minut.</p>

        <h4>5. Pytania i Odpowiedzi:</h4>

        <p><em>- Jak podczas czatu korzystać ze wskazanej strony internetowej?</em></p>

        <p>Jeżeli podczas czatu użyjemy znaku # a po nim adresu internetowego np.
        <pre>#https://pl.wikipedia.org/wiki/Adam_Wac%C5%82aw_cieszy%C5%84ski</pre> to po zatwierdzeniu przez klawisz Enter aplikacja postara się wczytać treść tej strony do kontekstu przekazywanego modelowi. Będzie wówczas można zadawać pytania dotyczące tej strony.</p>

        <p><em>- Jak skłonić czat do korzystania z internetu podczas przygotowywania odpowiedzi?</em></p>

        <p>Po lewej stronie pola na treść pytania/promptu znajduje się przycisk '+', p jego wciśnięciu rozwijane jest menu pozwalające dodawać pliki do rozmowy (opcja: Przeslij pliki), lub właśnie włączyć korzystanie z internetu (Wyszukiwarka).</p>

        <p><em>- Jak załączyć kolekcję dokumentów (bazę wiedzy) do rozmowy?</em></p>

        <p>Znak # wpisany w polu promptu wywołuje listę zdefiniowanych kolekcji, które można wskazać. Podczas przygotowywania odpowiedzi aplikacja najpierw przeszuka wskazaną kolekcję i udostępni najbardziej pasujące do treści promptu fragmenty dokumentów do kontekstu modelu. Wskazana kolekcja będzie wykorzystywana podczas całej rozmowy.</p>

        <p><em>- Czym różni się załączanie kolekcji dokumentów w czacie od korzystania z 'dostosowanych' modeli?</em></p>

        <p>Jeżeli chodzi o efekt to właściwie niczym, model 'dostosowany' ma wybrany z góry rzeczywisty model LLM, już ustawione kolekcje dokumentów z których będzie korzystał, ma zdefiniowany prompt systemowy i podpowiada typowe pytania pytania, które można mu zadać jednym kliknięciem. Może też mieć zmodyfikowane parametry pracy modelu. To samo można osiągnąć tworząc zwykłą nową rozmowę, wybierając model LLM z listy, modyfikując parametry modelu w parametrach rozmowy, przypisując kolekcje dokumentów poprzez znak # w polu promptu. Model 'dostosowany' jest po prostu wygodniejszy do konkretnego celu, np. pytań o regulaminy.</p>

        <p><em>- Skąd się biorą tytuły czatów z użytkownikiem zapisywanych przez OpenWebUI?</em></p>

        <p>Powstają automatycznie na podstawie treści rozmowy. Ale można je edytować.</p>


        <h4>Zastrzeżenia</h4>

        <p>LLMy mogą popełniać błędy. Zweryfikuj ważne informacje. Instytut Historii PAN nie ponosi odpowiedzialności ani za treść zapytań (lub załączonych do pytania dokumentów) wysyłanych do modeli przez użytkowników ani za treść odpowiedzi udzielanych przez modele LLM.</p>

        </div>
    </div>

    <hr>

    <div class="row">
        <div class="9 col">
            <p>Powrót na <a href="https://ai.ihpan.edu.pl">główną stronę</a> serwera.</p>
        </div>
    </div>


</body>
</html>


